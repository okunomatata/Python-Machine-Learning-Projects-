{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c8bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "training_data = pd.read_csv(\"bonus_train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91effadf",
   "metadata": {},
   "source": [
    "## Create training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a02d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = training_data.iloc[:,0]\n",
    "y_train = training_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd624c",
   "metadata": {},
   "source": [
    "## Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8eb2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch= tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2e767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare strings by removing certain characters and Tokenize the data \n",
    "def preprocess(X_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43ce539",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7294fee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for review in X_train:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4385ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "#Keep only the 10000 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa481e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are replacing the words with their respective ID\n",
    "#We also create a lookup table using 1000 Out of vocab words\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bba156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding here\n",
    "def encode_words(X_batch):\n",
    "    return table.lookup(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eee47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encode_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39ce619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6090, 32), dtype=int64, numpy=\n",
       "array([[ 7225,    10,  4578, ...,     0,     0,     0],\n",
       "       [ 7228,  7229,  7230, ...,     0,     0,     0],\n",
       "       [ 2256,  3408,     6, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [10042, 10670,   126, ...,     0,     0,     0],\n",
       "       [  879,   136,     5, ...,     0,     0,     0],\n",
       "       [    9,  3390,     6, ...,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5e291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "6085    0\n",
       "6086    0\n",
       "6087    0\n",
       "6088    1\n",
       "6089    1\n",
       "Name: target, Length: 6090, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cec8ef",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c13ab095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "191/191 [==============================] - 9s 29ms/step - loss: 0.5438 - accuracy: 0.7287\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 5s 29ms/step - loss: 0.2724 - accuracy: 0.8910\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 6s 29ms/step - loss: 0.1293 - accuracy: 0.9562\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 6s 29ms/step - loss: 0.0725 - accuracy: 0.9773\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 6s 29ms/step - loss: 0.0492 - accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "#Setting mask_zero to True to train model ignore padding tokens with id 0.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, \n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df8f5f",
   "metadata": {},
   "source": [
    "We see that our model has a very high accuracy and has learned the training data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504a4dc",
   "metadata": {},
   "source": [
    "## Using Pretrained Embedding for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8216ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\matthew\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\matthew\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.19.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\matthew\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model2 = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2783f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "191/191 [==============================] - 0s 684us/step - loss: 0.5614 - accuracy: 0.7251\n",
      "Epoch 2/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4988 - accuracy: 0.7632\n",
      "Epoch 3/25\n",
      "191/191 [==============================] - 0s 663us/step - loss: 0.4886 - accuracy: 0.7716\n",
      "Epoch 4/25\n",
      "191/191 [==============================] - 0s 674us/step - loss: 0.4818 - accuracy: 0.7780\n",
      "Epoch 5/25\n",
      "191/191 [==============================] - 0s 663us/step - loss: 0.4757 - accuracy: 0.7805\n",
      "Epoch 6/25\n",
      "191/191 [==============================] - 0s 684us/step - loss: 0.4702 - accuracy: 0.7847\n",
      "Epoch 7/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4653 - accuracy: 0.7862\n",
      "Epoch 8/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4601 - accuracy: 0.7859\n",
      "Epoch 9/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4555 - accuracy: 0.7877\n",
      "Epoch 10/25\n",
      "191/191 [==============================] - 0s 700us/step - loss: 0.4513 - accuracy: 0.7934\n",
      "Epoch 11/25\n",
      "191/191 [==============================] - 0s 858us/step - loss: 0.4472 - accuracy: 0.7970\n",
      "Epoch 12/25\n",
      "191/191 [==============================] - 0s 753us/step - loss: 0.4417 - accuracy: 0.7993\n",
      "Epoch 13/25\n",
      "191/191 [==============================] - 0s 674us/step - loss: 0.4379 - accuracy: 0.8025\n",
      "Epoch 14/25\n",
      "191/191 [==============================] - 0s 669us/step - loss: 0.4335 - accuracy: 0.8044\n",
      "Epoch 15/25\n",
      "191/191 [==============================] - 0s 669us/step - loss: 0.4305 - accuracy: 0.8072\n",
      "Epoch 16/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4248 - accuracy: 0.8118\n",
      "Epoch 17/25\n",
      "191/191 [==============================] - 0s 742us/step - loss: 0.4213 - accuracy: 0.8090\n",
      "Epoch 18/25\n",
      "191/191 [==============================] - 0s 821us/step - loss: 0.4166 - accuracy: 0.8113\n",
      "Epoch 19/25\n",
      "191/191 [==============================] - 0s 674us/step - loss: 0.4132 - accuracy: 0.8159\n",
      "Epoch 20/25\n",
      "191/191 [==============================] - 0s 674us/step - loss: 0.4100 - accuracy: 0.8169\n",
      "Epoch 21/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.4052 - accuracy: 0.8204\n",
      "Epoch 22/25\n",
      "191/191 [==============================] - 0s 663us/step - loss: 0.4029 - accuracy: 0.8212\n",
      "Epoch 23/25\n",
      "191/191 [==============================] - 0s 679us/step - loss: 0.3992 - accuracy: 0.8228\n",
      "Epoch 24/25\n",
      "191/191 [==============================] - 0s 684us/step - loss: 0.3956 - accuracy: 0.8238\n",
      "Epoch 25/25\n",
      "191/191 [==============================] - 0s 669us/step - loss: 0.3916 - accuracy: 0.8284\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_batch, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1391f",
   "metadata": {},
   "source": [
    "The pretrained embedding doesnt have as high of a score on training accuracy, but I was able to run \n",
    "many more epochs in a much shorter amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f285f22",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e73ba794",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"bonus_test_data.csv\")\n",
    "\n",
    "\n",
    "X_test = testing_data.iloc[:,0]\n",
    "y_test = testing_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0291e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 596us/step - loss: 0.4898 - accuracy: 0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48978060483932495, 0.7767564058303833]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa6cd18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf1e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess(X_test)\n",
    "X_test = encode_words(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aaba120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9824 - accuracy: 0.7223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9823856353759766, 0.7222586870193481]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc29da8",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364ad7d",
   "metadata": {},
   "source": [
    "We see that our model performs worse on accuracy and has a high loss when evaluating suggesting we overfit our dataset. \n",
    "The pretrained embedded model performs better and has a lower loss which shows how using pretrained modules can show improvment.\n",
    "\n",
    "Recommendations to improve the model might be to ad regulization or to tokenize the preprocess the texts to improve training.\n",
    "Because of my limited knowledge of NLP and Sentiment analysis, I had a difficult time exploring alternative methods of Tokenizing and encoding my data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
